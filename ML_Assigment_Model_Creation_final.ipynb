{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b0d0121",
   "metadata": {},
   "source": [
    "<h1><center>Prediction of YouTube Video Popularity</center></h1>\n",
    "<center></center>\n",
    "<ol> \n",
    "    <center>Orfeas Bourchas PhD StudentSchool of Naval Architecture & Marine Engineering NTUA</center>\n",
    "    <center>Nikos Silionis PhD StudentSchool of Naval Architecture & Marine Engineering NTUA</center>\n",
    "    <center>Dimitris Tsoumpelis PhD StudentSchool of Naval Architecture & Marine Engineering NTUA</center>\n",
    "<ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba0edb",
   "metadata": {},
   "source": [
    "<h2><center> Model Creating and Training for the Quantitative Variables</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21a8db",
   "metadata": {},
   "source": [
    "<h2><center>Importing Libraries </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa751ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Analysis Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Scikit Learn Modules\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import f1_score,  accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Tensorflow Library\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Optuna Library for parameter optimization\n",
    "import optuna \n",
    "\n",
    "# Set seed_value to be able to reproduce the results\n",
    "seed_value = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ed6fb",
   "metadata": {},
   "source": [
    "<h2><center> Load the Dataset</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f828c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('D:\\PhD_Material\\PhD_MSc\\Machine_Learning\\ML_Homeworks\\Homework_2\\Data')\n",
    "\n",
    "df = pd.read_csv('Model_Dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f0ac9",
   "metadata": {},
   "source": [
    "<h2><center> Create Variable Combinations </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0618f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = ['views', 'likes', 'dislikes', 'comment_count','days_till_trending','number_of_tags','title_number_of_trending_times', 'class']\n",
    "list_2 = ['views', 'days_till_trending', 'number_of_tags',    'title_number_of_trending_times', 'title_views_dif', 'class']\n",
    "list_3 = ['views', 'likes', 'class']\n",
    "list_4 = ['views', 'likes', 'dislikes', 'comment_count',\n",
    "       'days_till_trending', 'number_of_tags',\n",
    "       'title_number_of_trending_times', 'video_id_number_of_trending_times',\n",
    "       'title_dislikedf', 'video_id_likedf', 'video_id_dislikedf',\n",
    "       'title_tagsdif', 'video_id_tagsdif', 'title_time_dif',\n",
    "       'title_views_dif', 'video_id_time_dif', 'video_id_views_dif',\n",
    "       'title_comment_count_dif', 'video_id_comment_count_dif',\n",
    "       'comments_to_views', 'likes_to_dislikes', 'class']\n",
    "input_list = [list_1, list_2, list_3, list_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1812273",
   "metadata": {},
   "source": [
    "<h2><center> Create Dataset Creation function </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creation(df,columns):\n",
    "    df1 = df[columns].copy()\n",
    "    split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.3, random_state = seed_value)\n",
    "    for train_index,test_index in split.split(df1,df1[\"class\"]):\n",
    "        train_set_strat = df1.copy().loc[train_index]\n",
    "        test_set_strat = df1.copy().loc[test_index]\n",
    "    train_set = train_set_strat.copy()\n",
    "    test_set =test_set_strat.copy()\n",
    "    train_set_data = train_set.copy().drop(columns = 'class')\n",
    "    train_set_labels = train_set.copy()[\"class\"]\n",
    "    test_set_data = test_set.copy().drop(columns = 'class')\n",
    "    test_set_labels = test_set.copy()[\"class\"]\n",
    "    scaler = MinMaxScaler()\n",
    "    pipe = Pipeline([('scale', scaler)])\n",
    "\n",
    "    train_set_data_tr = pipe.fit_transform(train_set_data)\n",
    "    test_set_data_tr = pipe.transform(test_set_data)\n",
    "    train_set_labels_np = train_set_labels.to_numpy()\n",
    "    test_set_labels_np = test_set_labels.to_numpy()\n",
    "    return train_set_data_tr, test_set_data_tr, train_set_labels_np, test_set_labels_np, pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a16d2ca",
   "metadata": {},
   "source": [
    "<h2><center> Train Gaussian Naive Bays classifier </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6185a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response_accuracy = {}\n",
    "\n",
    "for i, name in enumerate(input_list):\n",
    "    train_set_data_tr, test_set_data_tr, train_set_labels_np, test_set_labels_np, pipe = dataset_creation(df,name)\n",
    "    \n",
    "    # Initialize the gnb classifier object.\n",
    "    gnb = GaussianNB()\n",
    "    \n",
    "    # Fit the training data to the classifier\n",
    "    model = gnb.fit(train_set_data_tr, train_set_labels_np)\n",
    "    y_pred = model.predict(test_set_data_tr)\n",
    "    print(classification_report(test_set_labels_np, y_pred))\n",
    "    \n",
    "    # Estimate the accuracy.\n",
    "    response_accuracy[f'gaussia naive bayes_{i+1}'] = round(gnb.score(test_set_data_tr, test_set_labels_np),3)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay.from_predictions(test_set_labels_np, y_pred)\n",
    "    disp.figure_.suptitle(f\"GNB Confusion Matrix for the {i+1} combination\")\n",
    "    plt.savefig(f\"GNB Confusion Matrix for the {i+1} combination.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d165467",
   "metadata": {},
   "source": [
    "<h1><center> Optuna Optimization </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6e982",
   "metadata": {},
   "source": [
    "<h2><center> SVC Objective Function </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_SVC(trial):\n",
    "    \n",
    "    model = SVC()\n",
    "    C = trial.suggest_loguniform('C', 1e-10, 1)\n",
    "    kernel = trial.suggest_categorical('kernel',['poly','rbf','sigmoid'])\n",
    "    degree = trial.suggest_int('degree',3, 8)\n",
    "    params = {\n",
    "        'C': C,\n",
    "        'kernel':kernel,\n",
    "        'degree': degree,\n",
    "    }\n",
    "    model.set_params(**params)\n",
    "    model.fit(train_set_data_tr, train_set_labels_np)\n",
    "\n",
    "    accuracy = model.score(test_set_data_tr, test_set_labels_np) \n",
    "    print(\"Train Score:\",model.score(test_set_data_tr, test_set_labels_np))\n",
    "    print(\"\\n=================\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de17f93",
   "metadata": {},
   "source": [
    "<h2><center> Running SVC Optimization </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(input_list):\n",
    "    study_name = f\"studies//SVC_study_{i}\"  \n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    train_set_data_tr, test_set_data_tr, train_set_labels_np, test_set_labels_np, pipe = dataset_creation(df,name)\n",
    "    study = optuna.create_study(directions=[\"maximize\"],study_name=study_name, storage=storage_name)\n",
    "    study.optimize(objective_SVC, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1481a",
   "metadata": {},
   "source": [
    "<h2><center> MPL Model Creation and Objective Function </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb3693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(trial):\n",
    "    # We optimize the numbers of layers and their units.\n",
    "    num_layers = trial.suggest_int(\"num_layers\", low = 1, high = 4, step = 1)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(train_set_data_tr.shape[1]))\n",
    "    model.add(Dense(5*train_set_data_tr.shape[1],activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=tf.keras.regularizers.l2(0.0)))\n",
    "    for i in range(num_layers):\n",
    "        neurons = trial.suggest_int(f\"neurons_{i}\", low = 120, high = 240, step = 120)\n",
    "        model.add(Dense(neurons, activation=\"relu\",kernel_initializer=\"he_normal\",kernel_regularizer=tf.keras.regularizers.l2(0.0)))\n",
    "    model.add(Dense(80,activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=tf.keras.regularizers.l2(0.0)))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    return model\n",
    "def create_optimizer(trial):\n",
    "    kwargs = {}\n",
    "    kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    optimizer = getattr(tf.optimizers, \"Adam\")(**kwargs)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d06fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_MLP(trial):\n",
    "    \n",
    "    model = create_classifier(trial)\n",
    "    optimizer = create_optimizer(trial)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    epochs = trial.suggest_int(\"epochs\", low = 100, high = 200, step = 100)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", low = 10, high = 50, step = 10)\n",
    "    model.fit( tf.convert_to_tensor(train_set_data_tr), train_set_labels_np, epochs=epochs, batch_size=batch_size)\n",
    "    accuracy = model.evaluate(test_set_data_tr, test_set_labels_np)[1]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3080a",
   "metadata": {},
   "source": [
    "<h2><center> Running MPL Optimization </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9960ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(input_list):\n",
    "    study_name = f\"MLP_study_{i}\"  # Unique identifier of the study.\n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    train_set_data_tr, test_set_data_tr, train_set_labels_np, test_set_labels_np, pipe = dataset_creation(df,name)\n",
    "    study = optuna.create_study(directions=[\"maximize\"],study_name=study_name, storage=storage_name)\n",
    "    study.optimize(objective_MLP, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6369e0d",
   "metadata": {},
   "source": [
    "<h1><center> Recreating Best models </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982f456",
   "metadata": {},
   "source": [
    "<h2><center>SVC</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(input_list):\n",
    "    comb = i+1\n",
    "    # Loading the study\n",
    "    study_name = f\"studies//SVC_study_{i}\"  \n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    \n",
    "    # Loading best trial parameters\n",
    "    C = loaded_study.best_trials[0].params['C']\n",
    "    degree = loaded_study.best_trials[0].params['degree']\n",
    "    kernel= loaded_study.best_trials[0].params['kernel']\n",
    "\n",
    "    # Reinitialize the model\n",
    "    params = {\n",
    "        'C':C,\n",
    "        'degree':degree,\n",
    "\n",
    "        'kernel':kernel\n",
    "        }\n",
    "    model = SVC()  \n",
    "    model.set_params(**params)\n",
    "    \n",
    "    # Retrain the best model\n",
    "    train_set_data_tr, test_set_data_tr, train_set_labels_np, test_set_labels_np, pipe = dataset_creation(df,name)\n",
    "    model.fit(train_set_data_tr, train_set_labels_np)\n",
    "    y_pred = model.predict(test_set_data_tr)\n",
    "    response_accuracy[f'SVC_{i+1}'] = round(model.score(test_set_data_tr, test_set_labels_np),3)\n",
    "    # Create the Confusion Matrix for each model\n",
    "    disp = metrics.ConfusionMatrixDisplay.from_predictions(test_set_labels_np, y_pred)\n",
    "    disp.figure_.suptitle(f\"SVC Confusion Matrix for the {i+1} combination\")\n",
    "    plt.savefig(f\"Confusion_Matrices//SVC Confusion Matrix for the {i+1} combination.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef2efa",
   "metadata": {},
   "source": [
    "<h2><center>MPL</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b75f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(input_list):\n",
    "    print(i)\n",
    "    comb = i+1\n",
    "    # Loading the study\n",
    "    study_name = f\"MLP_study_{i}\"  \n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    \n",
    "    # Loading best trial parameters and initializing the model\n",
    "    train_set_data_tr, test_set_data_tr, train_set_labels_np, test_set_labels_np, pipe = dataset_creation(df,name)\n",
    "    num_layers = loaded_study.best_trials[0].params['num_layers']\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(train_set_data_tr.shape[1]))\n",
    "    model.add(Dense(5*train_set_data_tr.shape[1],activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=tf.keras.regularizers.l2(0.0)))\n",
    "    for i in range(num_layers):\n",
    "        neurons = loaded_study.best_trials[0].params[f\"neurons_{i}\"]\n",
    "        model.add(Dense(neurons, activation=\"relu\",kernel_initializer=\"he_normal\",kernel_regularizer=tf.keras.regularizers.l2(0.0)))\n",
    "    model.add(Dense(80,activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=tf.keras.regularizers.l2(0.0)))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    kwargs = {}\n",
    "    kwargs[\"learning_rate\"] = loaded_study.best_trials[0].params[\"adam_learning_rate\"]\n",
    "    optimizer = getattr(tf.optimizers, \"Adam\")(**kwargs)\n",
    "    epochs = loaded_study.best_trials[0].params[\"epochs\"]\n",
    "    batch_size = loaded_study.best_trials[0].params[\"batch_size\"]\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    # Retrain the best model\n",
    "\n",
    "    model.fit( tf.convert_to_tensor(train_set_data_tr), train_set_labels_np, epochs=epochs, batch_size=300)\n",
    "    response_accuracy[f'MLP_{i}'] = round(model.evaluate(test_set_data_tr, test_set_labels_np)[1],3)\n",
    "    y_pred = np.argmax(model.predict(test_set_data_tr),axis=1)\n",
    "    # Create the Confusion Matrix for each model\n",
    "    disp = metrics.ConfusionMatrixDisplay.from_predictions(test_set_labels_np, y_pred)\n",
    "    disp.figure_.suptitle(f\"MPL Confusion Matrix for the {comb} combination\")\n",
    "    plt.savefig(f\"Confusion_Matrices//MPL Confusion Matrix for the {comb} combination.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38192716",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_sorted_keys_opt = sorted(response_accuracy, key=response_accuracy.get)  # [1, 3, 2]\n",
    "accuracy_sorted_dict_opt= {}\n",
    "for w in accuracy_sorted_keys_opt:\n",
    "    accuracy_sorted_dict_opt[w] = response_accuracy[w]\n",
    "\n",
    "names = list(accuracy_sorted_dict_opt.keys())\n",
    "accuracy_values = list(accuracy_sorted_dict_opt.values())\n",
    "\n",
    "\n",
    "x = np.arange(len(names))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,7))\n",
    "rects1 = ax.bar(x - width/2, accuracy_values, width, label='accuracy_score')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Trainining Scores accuracy')\n",
    "ax.set_xticks(x, names, rotation = 90)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "\n",
    "plt.savefig(f\"Confusion_Matrices//Accuracies.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb4b0b",
   "metadata": {},
   "source": [
    "<h2><center>Retrive the best parameters per input combination</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d811b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    study_name = f\"MLP_study_{i}\"  \n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    print(loaded_study.best_trials[0].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00446fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(i)\n",
    "    study_name = f\"studies//SVC_study_{i}\"  \n",
    "    storage_name = f\"sqlite:///{study_name}.db\"\n",
    "    loaded_study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    print(loaded_study.best_trials[0].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1135824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_Learning",
   "language": "python",
   "name": "machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
